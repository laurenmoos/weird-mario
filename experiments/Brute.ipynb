{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps =  1\n",
      "2\n",
      "new best reward -inf => 2\n",
      "timesteps =  1\n",
      "4\n",
      "new best reward 2 => 4\n",
      "timesteps =  2\n",
      "8\n",
      "new best reward 4 => 8\n",
      "timesteps =  2\n",
      "18\n",
      "new best reward 8 => 18\n",
      "timesteps =  3\n",
      "4\n",
      "timesteps =  3\n",
      "3\n",
      "timesteps =  4\n",
      "3\n",
      "timesteps =  4\n",
      "2\n",
      "timesteps =  5\n",
      "4\n",
      "timesteps =  5\n",
      "2\n",
      "timesteps =  6\n",
      "2\n",
      "timesteps =  6\n",
      "2\n",
      "timesteps =  7\n",
      "2\n",
      "timesteps =  7\n",
      "5\n",
      "timesteps =  8\n",
      "4\n",
      "timesteps =  8\n",
      "5\n",
      "timesteps =  9\n",
      "8\n",
      "timesteps =  9\n",
      "2\n",
      "timesteps =  10\n",
      "2\n",
      "timesteps =  10\n",
      "4\n",
      "timesteps =  11\n",
      "3\n",
      "timesteps =  11\n",
      "5\n",
      "timesteps =  12\n",
      "4\n",
      "timesteps =  12\n",
      "2\n",
      "timesteps =  13\n",
      "4\n",
      "timesteps =  13\n",
      "15\n",
      "timesteps =  14\n",
      "6\n",
      "timesteps =  14\n",
      "2\n",
      "timesteps =  15\n",
      "2\n",
      "timesteps =  15\n",
      "9\n",
      "timesteps =  16\n",
      "4\n",
      "timesteps =  16\n",
      "2\n",
      "timesteps =  17\n",
      "2\n",
      "timesteps =  17\n",
      "4\n",
      "timesteps =  18\n",
      "3\n",
      "timesteps =  18\n",
      "14\n",
      "timesteps =  19\n",
      "5\n",
      "timesteps =  19\n",
      "2\n",
      "timesteps =  20\n",
      "2\n",
      "timesteps =  20\n",
      "5\n",
      "timesteps =  21\n",
      "2\n",
      "timesteps =  21\n",
      "2\n",
      "timesteps =  22\n",
      "4\n",
      "timesteps =  22\n",
      "2\n",
      "timesteps =  23\n",
      "12\n",
      "timesteps =  23\n",
      "4\n",
      "timesteps =  24\n",
      "4\n",
      "timesteps =  24\n",
      "12\n",
      "timesteps =  25\n",
      "4\n",
      "timesteps =  25\n",
      "2\n",
      "timesteps =  26\n",
      "2\n",
      "timesteps =  26\n",
      "5\n",
      "timesteps =  27\n",
      "15\n",
      "timesteps =  27\n",
      "0\n",
      "timesteps =  28\n",
      "2\n",
      "timesteps =  28\n",
      "4\n",
      "timesteps =  29\n",
      "5\n",
      "timesteps =  29\n",
      "5\n",
      "timesteps =  30\n",
      "2\n",
      "timesteps =  30\n",
      "7\n",
      "timesteps =  31\n",
      "6\n",
      "timesteps =  31\n",
      "2\n",
      "timesteps =  32\n",
      "5\n",
      "timesteps =  32\n",
      "4\n",
      "timesteps =  33\n",
      "2\n",
      "timesteps =  33\n",
      "6\n",
      "timesteps =  34\n",
      "9\n",
      "timesteps =  34\n",
      "3\n",
      "timesteps =  35\n",
      "10\n",
      "timesteps =  35\n",
      "4\n",
      "timesteps =  36\n",
      "2\n",
      "timesteps =  36\n",
      "4\n",
      "timesteps =  37\n",
      "4\n",
      "timesteps =  37\n",
      "4\n",
      "timesteps =  38\n",
      "4\n",
      "timesteps =  38\n",
      "4\n",
      "timesteps =  39\n",
      "3\n",
      "timesteps =  39\n",
      "2\n",
      "timesteps =  40\n",
      "9\n",
      "timesteps =  40\n",
      "4\n",
      "timesteps =  41\n",
      "9\n",
      "timesteps =  41\n",
      "2\n",
      "timesteps =  42\n",
      "2\n",
      "timesteps =  42\n",
      "4\n",
      "timesteps =  43\n",
      "2\n",
      "timesteps =  43\n",
      "2\n",
      "timesteps =  44\n",
      "2\n",
      "timesteps =  44\n",
      "5\n",
      "timesteps =  45\n",
      "2\n",
      "timesteps =  45\n",
      "2\n",
      "timesteps =  46\n",
      "2\n",
      "timesteps =  46\n",
      "2\n",
      "timesteps =  47\n",
      "6\n",
      "timesteps =  47\n",
      "5\n",
      "timesteps =  48\n",
      "2\n",
      "timesteps =  48\n",
      "3\n",
      "timesteps =  49\n",
      "2\n",
      "timesteps =  49\n",
      "14\n",
      "timesteps =  50\n",
      "2\n",
      "timesteps =  50\n",
      "2\n",
      "timesteps =  51\n",
      "4\n",
      "timesteps =  51\n",
      "4\n",
      "timesteps =  52\n",
      "5\n",
      "timesteps =  52\n",
      "2\n",
      "timesteps =  53\n",
      "3\n",
      "timesteps =  53\n",
      "2\n",
      "timesteps =  54\n",
      "3\n",
      "timesteps =  54\n",
      "5\n",
      "timesteps =  55\n",
      "9\n",
      "timesteps =  55\n",
      "9\n",
      "timesteps =  56\n",
      "5\n",
      "timesteps =  56\n",
      "4\n",
      "timesteps =  57\n",
      "4\n",
      "timesteps =  57\n",
      "7\n",
      "timesteps =  58\n",
      "11\n",
      "timesteps =  58\n",
      "5\n",
      "timesteps =  59\n",
      "6\n",
      "timesteps =  59\n",
      "4\n",
      "timesteps =  60\n",
      "5\n",
      "timesteps =  60\n",
      "3\n",
      "timesteps =  61\n",
      "2\n",
      "timesteps =  61\n",
      "2\n",
      "timesteps =  62\n",
      "9\n",
      "timesteps =  62\n",
      "2\n",
      "timesteps =  63\n",
      "2\n",
      "timesteps =  63\n",
      "5\n",
      "timesteps =  64\n",
      "11\n",
      "timesteps =  64\n",
      "3\n",
      "timesteps =  65\n",
      "5\n",
      "timesteps =  65\n",
      "5\n",
      "timesteps =  66\n",
      "2\n",
      "timesteps =  66\n",
      "2\n",
      "timesteps =  67\n",
      "2\n",
      "timesteps =  67\n",
      "3\n",
      "timesteps =  68\n",
      "2\n",
      "timesteps =  68\n",
      "2\n",
      "timesteps =  69\n",
      "2\n",
      "timesteps =  69\n",
      "4\n",
      "timesteps =  70\n",
      "6\n",
      "timesteps =  70\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Implementation of the Brute from \"Revisiting the Arcade Learning Environment:\n",
    "Evaluation Protocols and Open Problems for General Agents\" by Machado et al.\n",
    "https://arxiv.org/abs/1709.06009\n",
    "This is an agent that uses the determinism of the environment in order to do\n",
    "pretty well at a number of retro games.  It does not save emulator state but\n",
    "does rely on the same sequence of actions producing the same result when played\n",
    "back.\n",
    "\"\"\"\n",
    "\n",
    "import random\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import retro\n",
    "import gym\n",
    "\n",
    "\n",
    "EXPLORATION_PARAM = 0.05\n",
    "\n",
    "\n",
    "class StochasticFrameSkip(gym.Wrapper):\n",
    "    def __init__(self, env, n, stickprob):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.n = n\n",
    "        self.stickprob = stickprob\n",
    "        self.curac = None\n",
    "        self.rng = np.random.RandomState()\n",
    "        self.supports_want_render = hasattr(env, \"supports_want_render\")\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        self.curac = None\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "    def step(self, ac):\n",
    "        done = False\n",
    "        totrew = 0\n",
    "        for i in range(self.n):\n",
    "            # First step after reset, use action\n",
    "            if self.curac is None:\n",
    "                self.curac = ac\n",
    "            # First substep, delay with probability=stickprob\n",
    "            elif i==0:\n",
    "                if self.rng.rand() > self.stickprob:\n",
    "                    self.curac = ac\n",
    "            # Second substep, new action definitely kicks in\n",
    "            elif i==1:\n",
    "                self.curac = ac\n",
    "            if self.supports_want_render and i<self.n-1:\n",
    "                ob, rew, done, info = self.env.step(self.curac, want_render=False)\n",
    "            else:\n",
    "                ob, rew, done, info = self.env.step(self.curac)\n",
    "            totrew += rew\n",
    "            if done: break\n",
    "        return ob, totrew, done, info\n",
    "\n",
    "    def seed(self, s):\n",
    "        self.rng.seed(s)\n",
    "\n",
    "class SnesDiscretizer(gym.ActionWrapper):\n",
    "    \"\"\"\n",
    "    Wrap a gym-retro environment and make it use discrete\n",
    "    actions for the Sonic game.\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        super(SnesDiscretizer, self).__init__(env)\n",
    "        buttons = [\"B\", \"Y\", \"SELECT\", \"START\", \"up\", \"down\", \"left\", \"right\", \"A\", \"X\", \"L\", \"R\"]\n",
    "        actions = [['right'],['right', 'A'],['right', 'B'],['right','Y'],['A'],['B'],['left'],['left', 'A'],\n",
    "                   ['left', 'B'],['left','Y'],['A','Y'],['B','Y'],['down'],['up'], ['Y', 'up'],['B','up'], ['A','up'],['A','Y','right'], \n",
    "                  ['A','Y','left'],['B','Y','right'],['B','Y','left'],['SELECT']]\n",
    "      \n",
    "        \n",
    "        self._actions = []\n",
    "        for action in actions:\n",
    "            arr = np.array([False] * 12)\n",
    "            for button in action:\n",
    "                arr[buttons.index(button)] = True\n",
    "            self._actions.append(arr)\n",
    "        self.action_space = gym.spaces.Discrete(len(self._actions))\n",
    "\n",
    "    def action(self, a): # pylint: disable=W0221\n",
    "        return self._actions[a].copy()\n",
    "    \n",
    "class ProcessFrameMario(gym.Wrapper):\n",
    "    def __init__(self, env=None, reward_type=None, dim=84):\n",
    "        super(ProcessFrameMario, self).__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(1, dim, dim), dtype=np.uint8)\n",
    "        \n",
    "        self.timer = 90000\n",
    "        self.countdown = 0\n",
    "        self.multiplier = 0\n",
    "        self.fresh= True\n",
    "        self.x = 0\n",
    "        self.s = 0\n",
    "        self.code_covered = set()\n",
    "        self.crashed = False \n",
    "        \n",
    "    def step(self, action): #pylint: disable=method-hidden\n",
    "            \n",
    "        \n",
    "        if  self.timer % 50 == 0:\n",
    "                retro._retro.Memory.assign(self.env.data.memory, 8261058, \"uint8\", 1) \n",
    "            \n",
    "\n",
    "        if self.fresh: \n",
    "\n",
    "            retro._retro.Memory.assign(self.env.data.memory, 8257561, \"uint8\", 22)\n",
    "            retro._retro.Memory.assign(self.env.data.memory, 8261058, \"uint8\", 1) \n",
    "\n",
    "            self.fresh = False\n",
    "        \n",
    "        #action =  self.env.action_space.sample()\n",
    "        obs, _, done, info = self.env.step(action)\n",
    "      \n",
    "        \n",
    "        if (info ['powerup'] != 22) and (info['powerup']>3):\n",
    "            self.crashed = True\n",
    "         \n",
    "       \n",
    "        if self.crashed:\n",
    "            info['crash'] = 1\n",
    "            \n",
    "        else:\n",
    "            info ['crash'] = 0\n",
    "         \n",
    "        self.timer-=1\n",
    "        \n",
    "        reward = 0\n",
    "      \n",
    "        trace = info ['trace'][:1000]\n",
    "        line = [x[2] for x in trace]\n",
    "        for word in line:\n",
    "            if word not in self.code_covered:  \n",
    "                self.code_covered.add(word)\n",
    "                #reward+=1\n",
    "                reward = 1 \n",
    "                \n",
    "                \n",
    "        if reward == 0:\n",
    "            self.countdown += 1\n",
    "        else:\n",
    "            self.countdown = 0\n",
    "            self.multiplier+=1\n",
    "            #reward = self.multiplier      \n",
    "        \n",
    "        if self.countdown > 200:\n",
    "            done = True\n",
    "            \n",
    "        #if self.timer ==0:\n",
    "            #done = True\n",
    "            \n",
    "       \n",
    "        \n",
    "        if done:     \n",
    "            self.timer = 90000\n",
    "            self.fresh = True \n",
    "            self.x = 0\n",
    "            self.s = 0\n",
    "            self.countdown = 0\n",
    "            self.code_covered = set()\n",
    "            self.crashed = False \n",
    "            self.multiplier = 0\n",
    "    \n",
    "        \n",
    "        \n",
    "        #return obs, (reward*reward)/500, done, info\n",
    "        return obs, reward, done, info\n",
    "\n",
    "\n",
    "class Frameskip(gym.Wrapper):\n",
    "    def __init__(self, env, skip=4):\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "        #self.x = 0\n",
    "\n",
    "    def reset(self):\n",
    "        return self.env.reset()\n",
    "\n",
    "    def step(self, act):\n",
    "        total_rew = 0.0\n",
    "        done = None\n",
    "        for i in range(self._skip):\n",
    "            obs, rew, done, info = self.env.step(act)\n",
    "            \n",
    "            \"\"\"\n",
    "            rew = 0\n",
    "            if info['x']> self.x:\n",
    "                rew = info['x'] - self.x\n",
    "                self.x = info['x']\n",
    "            \"\"\"\n",
    "            \n",
    "            total_rew += rew\n",
    "            if done:\n",
    "                self.x = 0 \n",
    "                break\n",
    "\n",
    "        return obs, total_rew, done, info\n",
    "\n",
    "\n",
    "class TimeLimit(gym.Wrapper):\n",
    "    def __init__(self, env, max_episode_steps=None):\n",
    "        super().__init__(env)\n",
    "        self._max_episode_steps = max_episode_steps\n",
    "        self._elapsed_steps = 0\n",
    "\n",
    "    def step(self, ac):\n",
    "        observation, reward, done, info = self.env.step(ac)\n",
    "        self._elapsed_steps += 1\n",
    "        if self._elapsed_steps >= self._max_episode_steps:\n",
    "            done = True\n",
    "            info['TimeLimit.truncated'] = True\n",
    "        return observation, reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        self._elapsed_steps = 0\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, value=-np.inf, children=None):\n",
    "        self.value = value\n",
    "        self.visits = 0\n",
    "        self.children = {} if children is None else children\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"<Node value=%f visits=%d len(children)=%d>\" % (\n",
    "            self.value,\n",
    "            self.visits,\n",
    "            len(self.children),\n",
    "        )\n",
    "\n",
    "\n",
    "def select_actions(root, action_space, max_episode_steps):\n",
    "    \"\"\"\n",
    "    Select actions from the tree\n",
    "    Normally we select the greedy action that has the highest reward\n",
    "    associated with that subtree.  We have a small chance to select a\n",
    "    random action based on the exploration param and visit count of the\n",
    "    current node at each step.\n",
    "    We select actions for the longest possible episode, but normally these\n",
    "    will not all be used.  They will instead be truncated to the length\n",
    "    of the actual episode and then used to update the tree.\n",
    "    \"\"\"\n",
    "    node = root\n",
    "\n",
    "    acts = []\n",
    "    steps = 0\n",
    "    while steps < max_episode_steps:\n",
    "        if node is None:\n",
    "            # we've fallen off the explored area of the tree, just select random actions\n",
    "            act = action_space.sample()\n",
    "        else:\n",
    "            epsilon = EXPLORATION_PARAM / np.log(node.visits + 2)\n",
    "            if random.random() < epsilon:\n",
    "                # random action\n",
    "                act = action_space.sample()\n",
    "            else:\n",
    "                # greedy action\n",
    "                act_value = {}\n",
    "                for act in range(action_space.n):\n",
    "                    if node is not None and act in node.children:\n",
    "                        act_value[act] = node.children[act].value\n",
    "                    else:\n",
    "                        act_value[act] = -np.inf\n",
    "                best_value = max(act_value.values())\n",
    "                best_acts = [\n",
    "                    act for act, value in act_value.items() if value == best_value\n",
    "                ]\n",
    "                act = random.choice(best_acts)\n",
    "\n",
    "            if act in node.children:\n",
    "                node = node.children[act]\n",
    "            else:\n",
    "                node = None\n",
    "\n",
    "        acts.append(act)\n",
    "        steps += 1\n",
    "\n",
    "    return acts\n",
    "\n",
    "\n",
    "def rollout(env, acts):\n",
    "    \"\"\"\n",
    "    Perform a rollout using a preset collection of actions\n",
    "    \"\"\"\n",
    "    total_rew = 0\n",
    "    env.reset()\n",
    "    steps = 0\n",
    "    for act in acts:\n",
    "        _obs, rew, done, _info = env.step(act)\n",
    "        \n",
    "\n",
    "        steps += 1\n",
    "        total_rew += rew\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return steps, total_rew\n",
    "\n",
    "\n",
    "def update_tree(root, executed_acts, total_rew):\n",
    "    \"\"\"\n",
    "    Given the tree, a list of actions that were executed before the game ended, and a reward, update the tree\n",
    "    so that the path formed by the executed actions are all updated to the new reward.\n",
    "    \"\"\"\n",
    "    root.value = max(total_rew, root.value)\n",
    "    root.visits += 1\n",
    "    new_nodes = 0\n",
    "\n",
    "    node = root\n",
    "    for step, act in enumerate(executed_acts):\n",
    "        if act not in node.children:\n",
    "            node.children[act] = Node()\n",
    "            new_nodes += 1\n",
    "        node = node.children[act]\n",
    "        node.value = max(total_rew, node.value)\n",
    "        node.visits += 1\n",
    "\n",
    "    return new_nodes\n",
    "\n",
    "\n",
    "class Brute:\n",
    "    \"\"\"\n",
    "    Implementation of the Brute\n",
    "    Creates and manages the tree storing game actions and rewards\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, max_episode_steps):\n",
    "        self.node_count = 1\n",
    "        self._root = Node()\n",
    "        self._env = env\n",
    "        self._max_episode_steps = max_episode_steps\n",
    "\n",
    "    def run(self):\n",
    "        acts = select_actions(self._root, self._env.action_space, self._max_episode_steps)\n",
    "        steps, total_rew = rollout(self._env, acts)\n",
    "        executed_acts = acts[:steps]\n",
    "        self.node_count += update_tree(self._root, executed_acts, total_rew)\n",
    "        return executed_acts, total_rew\n",
    "\n",
    "max_episode_steps = 800\n",
    "\n",
    "env = retro.make( game='SuperMarioWorld-Snes', state= 'YoshiIsland4.state', use_restricted_actions=retro.Actions.ALL)\n",
    "env = StochasticFrameSkip(env, n=4, stickprob=0.25)\n",
    "\n",
    "#env = Frameskip(env)\n",
    "#env = TimeLimit(env, max_episode_steps=max_episode_steps)\n",
    "env = SnesDiscretizer(env)\n",
    "env = ProcessFrameMario (env)\n",
    "\n",
    "timestep_limit=1e12\n",
    "\n",
    "brute = Brute(env, max_episode_steps=max_episode_steps)\n",
    "timesteps = 0\n",
    "best_rew = float('-inf')\n",
    "while True:\n",
    "    acts, rew = brute.run()\n",
    "    timesteps += 1\n",
    "    print (\"timesteps = \", timesteps)\n",
    "    print (rew)\n",
    "    if rew > best_rew:\n",
    "        print(\"new best reward {} => {}\".format(best_rew, rew))\n",
    "        best_rew = rew\n",
    "        #env.unwrapped.record_movie(\"best.bk2\")\n",
    "        env.reset()\n",
    "        #for act in acts:\n",
    "        #    env.step(act)\n",
    "        #env.unwrapped.stop_record()\n",
    "    acts, rew = brute.run()\n",
    "    print (\"timesteps = \", timesteps)\n",
    "    print (rew)\n",
    "    if rew > best_rew:\n",
    "        print(\"new best reward {} => {}\".format(best_rew, rew))\n",
    "        best_rew = rew\n",
    "        #env.unwrapped.record_movie(\"best.bk2\")\n",
    "        #for act in acts:\n",
    "        #    env.step(act)\n",
    "        #env.unwrapped.stop_record()\n",
    "    if timesteps > timestep_limit:\n",
    "        print(\"timestep limit exceeded\")\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
