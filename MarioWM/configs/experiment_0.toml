


[environment]
    name = 'experiment_0'
    seed = 42
    num_processes = 2
    #discount factor to apply to future rewards
    gamma=0.01
    use_weird= true
    with_authoshroom= true
    level = 4
    #once we eventually get to testing generalization
    validation_level = 8
    allow_early_resets = true
    episode_length = 90000
    #'frame skip' is the number of frames an action is repeated before a new action is selected and its rate
    #has been shown to be a powerful hyperparameter for RL performance in Atari games
    #http://nn.cs.utexas.edu/downloads/papers/braylan.aaai15.pdf
    use_skip = true
    reward = 0
    reward_trace = 200


[policy]

#Model 0: LSTM

# Model 1
# Conv inputs x 32 x 8 ReLu
# Conv 32 x 64 x 4 ReLu
# Conv 64 x 32 x 3 Relu
# Conv32 * 7 * 7, hidden size Relu

# Model 2
# linear layer

    base_network_type = 'CNN'
    base_network_topology = 1
    #model the policy with a recurrent neural network which results in a continuous representation
    use_recurrent_policy = true

[logging]
    log_interval = 5
    log_dir = '/tmp'
    eval_log_dir = '/tmp'
    save_dir = '/tmp'

#can include nested section for agent specific parameters
[agent]
    trace_size = 2000
    trace_length = 1000
    algo = "ppo"
    num_steps = 40
    use_acktr = true
    [a2c]
        #value function coefficient for the loss
        value_loss_coeff = 0.01
        #The entropy coefficient is a regularizer. A policy has maximum entropy when all policies are equally
        #likely and minimum when the one action probability of the policy is dominant. The entropy coefficient
        #is multiplied by the maximum possible entropy and added to loss. This helps prevent premature convergence
        #of one action probability dominating the policy and preventing exploration
        entropy_coef = 0.01
        #gradient clipping ensures the gradient vector g has norm at most equal to configured threshold
        max_grad_norm = 0.1
        #RMS prop decay parameter
        alpha = 0.01
        learning_rate = 2.5e-5
    [ppo]
        clip = 0.1
        epoch = 1000
        num_mini_batch = 5
        #mean loss of the value function update, corresponds to the accuracy of algorithhm predicting reward
        value_loss_coeff = 0.5
        #The entropy coefficient is a regularizer. A policy has maximum entropy when all policies are equally
        #likely and minimum when the one action probability of the policy is dominant. The entropy coefficient
        #is multiplied by the maximum possible entropy and added to loss. This helps prevent premature convergence
        #of one action probability dominating the policy and preventing exploration
        entropy = 0.01
        #gradient clipping ensures the gradient vector g has norm at most equal to configured threshold
        max_grad_norm = 0.1
        #restrict rewards to a constrained value, see https://arxiv.org/pdf/1602.07714.pdf
        use_clipped_value_loss = true

        learning_rate = 2.5e-5
