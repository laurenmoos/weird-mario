


[environment]
    name = 'experiment_0'
    seed = 1
    num_processes = 2
    #discount factor to apply to future rewards
    gamma=0.99
    use_weird= true
    with_authoshroom= true
    level = 2
    #once we eventually get to testing generalization
    validation_level = 8
    allow_early_resets = true
    episode_length = 200
    #'frame skip' is the number of frames an action is repeated before a new action is selected and its rate
    #has been shown to be a powerful hyperparameter for RL performance in Atari games
    #http://nn.cs.utexas.edu/downloads/papers/braylan.aaai15.pdf
    use_skip = true
    reward = 2
    reward_trace = 100
    env_steps = 10e6
    use_proper_time_limits = true


[policy]

#Model 0: LSTM

# Model 1
# Conv inputs x 32 x 8 ReLu
# Conv 32 x 64 x 4 ReLu
# Conv 64 x 32 x 3 Relu
# Conv32 * 7 * 7, hidden size Relu

# Model 2
# linear layer

    base_network_type = 'CNN'
    base_network_topology = 1
    #model the policy with a recurrent neural network which results in a continuous representation
    use_recurrent_policy = true

[logging]
    log_interval = 5
    log_dir = '/tmp'
    eval_log_dir = '/tmp'


#can include nested section for agent specific parameters
[agent]
    trace_size = 100
    eval_interval = 100
    save_interval = 100
    save_dir = '/tmp'
    algo = "ppo"
    num_updates = 100
    num_steps = 5
    use_acktr = true
    #a2c
    #value function (algorithm predicting reward) coefficient for the loss
    value_loss_coeff = 0.01
    #The entropy coefficient is a regularizer. A policy has maximum entropy when all policies are equally
    #likely and minimum when the one action probability of the policy is dominant. The entropy coefficient
    #is multiplied by the maximum possible entropy and added to loss. This helps prevent premature convergence
    #of one action probability dominating the policy and preventing exploration
    entropy_coef = 0.01
    #gradient clipping ensures the gradient vector g has norm at most equal to configured threshold
    max_grad_norm = 0.1
    #RMS prop decay parameter
    alpha = 0.99
    learning_rate = 7e-4
    #ppo
    is_headless = true
    clip = 0.1
    epoch = 1000
    num_mini_batch = 5
    #restrict rewards to a constrained value, see https://arxiv.org/pdf/1602.07714.pdf
    use_clipped_value_loss = true
    use_linear_lr_decay = true

